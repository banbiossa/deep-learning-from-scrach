{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.5_ss_chap5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src import utils\n",
    "import itertools\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "# logging.basicConfig(level=logging.DEBUG, format=log_fmt)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "        def __init__(self):\n",
    "                ''' Multiplication interface for backward propagation\n",
    "                Multiplication returns f(x,y) =x * y\n",
    "                The chain rule stands: df/dx = y, df/dy = x \n",
    "                Backward propagation is: dx = dout * df/dx = dout * y\n",
    "                Backward propagation is: dy = dout * df/dy = dout * x\n",
    "                \n",
    "                Properties\n",
    "                x: num\n",
    "                        Forward x\n",
    "                y: num\n",
    "                        Forward y\n",
    "                '''\n",
    "                self.x = None\n",
    "                self.y = None\n",
    "        \n",
    "        def forward(self, x, y):\n",
    "                ''' Forward method of multiplication\n",
    "                Initalized x, y on forward\n",
    "                '''\n",
    "                self.x = x\n",
    "                self.y = y\n",
    "                out = x * y\n",
    "                \n",
    "                return out\n",
    "        \n",
    "        def backward(self, dout):\n",
    "                ''' Backward method of multiplication\n",
    "                Chain rule follows that df/dx=y, df/dx=x\n",
    "                '''\n",
    "                dx = dout * self.y\n",
    "                dy = dout * self.x\n",
    "                \n",
    "                return dx, dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2\n"
     ]
    }
   ],
   "source": [
    "print(dapple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "print(dapple_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "        ''' add interface for backward propagation\n",
    "        add returns f(x,y) =x + y\n",
    "        The chain rule stands: df/dx = 1, df/dy = 1 \n",
    "        Backward propagation is: dx = dout * df/dx = dout * 1\n",
    "        Backward propagation is: dy = dout * df/dy = dout * 1\n",
    "\n",
    "        Properties\n",
    "        x: num\n",
    "                Forward x\n",
    "        y: num\n",
    "                Forward y\n",
    "        '''\n",
    "        def __init__(self):\n",
    "                pass\n",
    "        \n",
    "        def forward(self, x, y):\n",
    "                out = x + y\n",
    "                return out\n",
    "        \n",
    "        def backward(self, dout):\n",
    "                dx = dout * 1\n",
    "                dy = dout * 1\n",
    "                return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "orange = 150\n",
    "orange_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "orange_price = mul_orange_layer.forward(orange, orange_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = mul_tax_layer.forward(all_price, tax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprice = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dall_price, dtax = mul_tax_layer.backward(dprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n"
     ]
    }
   ],
   "source": [
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.00000000000001"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dapple_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dorange_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "        ''' Relu activation\n",
    "        f(x) = x (x > 0), 0 (x <=0)\n",
    "        df/dx = 1(x>0), 0(x<=0)\n",
    "        '''\n",
    "        def __init__(self):\n",
    "                self.mask = None\n",
    "        \n",
    "        def forward(self, x):\n",
    "                ''' output relu\n",
    "                \n",
    "                Parameters\n",
    "                ----------\n",
    "                x: numpy array\n",
    "                \n",
    "                Returns\n",
    "                -------\n",
    "                out: numpy array\n",
    "                        masked as 0 where x<=0\n",
    "                '''\n",
    "                self.mask = (x <= 0)\n",
    "                out = x.copy()\n",
    "                out[self.mask] = 0\n",
    "                \n",
    "                return out\n",
    "        \n",
    "        def backward(self, dout):\n",
    "                dout[self.mask] = 0\n",
    "                dx = dout\n",
    "                \n",
    "                return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "        ''' Sigmoid interface\n",
    "        y = f(x) = 1/(1+exp(-x))\n",
    "        dy/dx = exp(-x)/(1+exp(-x))^2 = y(1-y)\n",
    "        '''\n",
    "        def __init__(self):\n",
    "                self.out = None\n",
    "        \n",
    "        def forward(self, x):\n",
    "                out = 1/(1+np.exp(-x))\n",
    "                self.out = out\n",
    "                \n",
    "                return out\n",
    "        \n",
    "        def backward(self, dout):\n",
    "                dx = dout * (1.0 - self.out) * self.out\n",
    "                \n",
    "                return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "        ''' Interface for affine transformation (dot product)\n",
    "        f(x, W, b) = x %*% W + b\n",
    "        df/dx = W.T\n",
    "        df/dW = x.T\n",
    "        df/db = 1\n",
    "        \n",
    "        '''\n",
    "        def __init__(self, W, b):\n",
    "                self.W = W\n",
    "                self.x = None\n",
    "                self.dW = None\n",
    "                self.db = None\n",
    "        \n",
    "        def forward(self, x):\n",
    "                self.x = x\n",
    "                out = np.dot(x, self.W) + self.b\n",
    "                \n",
    "                return out\n",
    "        \n",
    "        def backward(self, dout):\n",
    "                dx = np.dot(dout, self.W.T)\n",
    "                self.dW = np.dot(self.x.T, dout)\n",
    "                self.db = np.sum(dout, axis=0)\n",
    "                \n",
    "                return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.functions import softmax, cross_entropy_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "        def __init__(self):\n",
    "                self.loss = None\n",
    "                self.y = None\n",
    "                self.t = None\n",
    "        \n",
    "        def forward(self, x, t):\n",
    "                self.t = t\n",
    "                self.y = softmax(x)\n",
    "                self.loss = cross_entropy_error(self.y, self.t)\n",
    "                \n",
    "                return self.loss\n",
    "        \n",
    "        def backward(self, dout=1):\n",
    "                batch_size = self.t.shape[0]\n",
    "                dx = (self.y - self.t)/batch_size\n",
    "                \n",
    "                return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.layers import *\n",
    "from src.common.gradient import numerical_gradient\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "        ''' Two layered net\n",
    "        '''\n",
    "        def __init__(self, input_size, hidden_size, output_size,\n",
    "                    weight_init_std = 0.01):\n",
    "                # Initialize\n",
    "                self.params={}\n",
    "                self.params['W1'] = weight_init_std * \\\n",
    "                        np.random.randn(input_size, hidden_size)\n",
    "                self.params['b1'] = np.zeros(hidden_size)\n",
    "                self.params['W2'] = weight_init_std * \\\n",
    "                        np.random.randn(hidden_size, output_size)\n",
    "                self.params['b2'] = np.zeros(output_size)\n",
    "                \n",
    "                # Layers\n",
    "                self.layers = OrderedDict()\n",
    "                self.layers['Affine1'] = Affine(self.params['W1'],\n",
    "                                               self.params['b1'])\n",
    "                self.layers['Relu1'] = Relu()\n",
    "                self.layers['Affine2'] = Affine(self.params['W2'],\n",
    "                                               self.params['b2'])\n",
    "                \n",
    "                self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "        def predict(self, x):\n",
    "                for layer in self.layers.values():\n",
    "                        x = layer.forward(x)\n",
    "                \n",
    "                return x\n",
    "        \n",
    "        # x: in, t: out\n",
    "        def loss(self, x, t):\n",
    "                y = self.predict(x)\n",
    "                return self.lastLayer.forward(y, t)\n",
    "        \n",
    "        def accuracy(self, x, t):\n",
    "                y = self.predict(x)\n",
    "                y = np.argmax(y, axis=1)\n",
    "                if t.ndim != 1: \n",
    "                        t = np.argmax(t, axis=1)\n",
    "                accuracy = np.sum(y==t)/float(x.shape[0])\n",
    "                \n",
    "                return accuracy\n",
    "        \n",
    "        def numerical_gradient(self, x, t):\n",
    "                loss_W = lambda W: self.loss(x, t)\n",
    "                \n",
    "                grads = {}\n",
    "                grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "                grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "                grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "                grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "                \n",
    "                return grads\n",
    "        \n",
    "        def gradient(self, x, t):\n",
    "                # forward\n",
    "                self.loss(x, t)\n",
    "                \n",
    "                # backward\n",
    "                dout = 1\n",
    "                dout = self.lastLayer.backward(dout)\n",
    "                \n",
    "                layers = list(self.layers.values())\n",
    "                layers.reverse()\n",
    "                for layer in layers:\n",
    "                        dout = layer.backward(dout)\n",
    "                        \n",
    "                # input\n",
    "                grads = {}\n",
    "                grads['W1'] = self.layers['Affine1'].dW\n",
    "                grads['b1'] = self.layers['Affine1'].db\n",
    "                grads['W2'] = self.layers['Affine2'].dW\n",
    "                grads['b2'] = self.layers['Affine2'].db\n",
    "                \n",
    "                return grads\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, \n",
    "                                                  one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_backprop = network.gradient(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-04 18:53:19,609 - src.common.layers - INFO - W1: 4.246875568409945e-10\n",
      "2018-11-04 18:53:19,610 - src.common.layers - INFO - b1: 2.4277005349551273e-09\n",
      "2018-11-04 18:53:19,613 - src.common.layers - INFO - W2: 5.425777944473201e-09\n",
      "2018-11-04 18:53:19,617 - src.common.layers - INFO - b2: 1.3944884606070795e-07\n"
     ]
    }
   ],
   "source": [
    "for key in grad_numerical.keys():\n",
    "        diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "        logger.info(f\"{key}: {diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
